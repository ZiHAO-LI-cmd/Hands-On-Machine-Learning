{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 梯度消失与梯度爆炸问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.1 Glorot 和 He 初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，Keras使用具有均匀分布的Glorot初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.2 非饱和激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 leaky ReLU 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     [...]\n",
    "#     keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "#     keras.layers.LeakyReLU(alpha=0.2)\n",
    "#     [...]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.3 批量归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个输入零中心并归一化，然后每层使用两个新的参数向量缩放和偏移其结果：一个用于缩放，另一个用于偏移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个隐藏层的激活函数之前或之后添加一个 BatchNormalization 层  \n",
    "\n",
    "keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.4 梯度裁剪 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播期间裁剪梯度，使它们永远不会超过某个阈值  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras中，创建优化器时设置clipvalue 或 clipnorm 参数，例如：  \n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)  \n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 重用预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务越相似，可重用的层越多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.1 用 Keras 进行迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已有模型A，加载模型A并基于该模型的层创建一个新模型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对模型A克隆，复制其权重\n",
    "# model_A_clone = keras.models.clone_model(model_A)\n",
    "# model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "# 重用除输出层外的所有层\n",
    "# model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.2 无监督预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在无监督训练中，使用一个无监督学习技术对无标记数据(或所有数据)进行训练，  \n",
    "然后使用一个有监督学习技术对有标记数据进行最后任务的微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 更快的优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.1 动量优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降等式：  \n",
    "$\\theta \\leftarrow \\theta-\\eta\\nabla_{\\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量优化关心先前的梯度是什么：在每次迭代时，从动量向量$m$ (乘以学习率$\\eta$) 中减去局部梯度，  \n",
    "并通过添加该动量向量来更新权重。  \n",
    "1. $m \\leftarrow \\beta m-\\eta\\nabla_{\\theta}J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 中实现：  \n",
    "使用SGD优化器并设置其超参数momentum  \n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.2 Nesterov 加速梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $\\theta + \\beta m$ 处沿动量方向稍微提前处测量成本函数的梯度  \n",
    "1. $m \\leftarrow \\beta m-\\eta\\nabla_{\\theta}J(\\theta + \\beta m)$\n",
    "2. $\\theta \\leftarrow \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 中实现：  \n",
    "使用SGD优化器并设置nesterov = True  \n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.3 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过沿最陡峭的维度按比例缩小梯度向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.4 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只累加最近迭代中的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.5 Adam 和 Nadam 优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam 代表自适应矩估计，结合了动量优化和 RMSProp 的思想：  \n",
    "像动量优化一样，追踪过去梯度的指数衰减平均值  \n",
    "像RMSProp一样，追踪过去平方梯度的指数衰减平均值  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.6 学习率调度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4 通过正则化避免过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d99a3f7b344b3c3107482760db15f42178bfad658d282ab0a919b76809e13cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
