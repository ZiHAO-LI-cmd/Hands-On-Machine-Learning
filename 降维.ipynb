{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 维度的诅咒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 高维数据集很大可能是非常稀疏的：大多数训练实例可能彼此之间相距很远\n",
    "* 训练集的维度越高，过拟合的风险就越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 降维的主要方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有训练实例都位于(或接近于)高维空间的低维子空间内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "投影并不只是降低尺寸的最佳方法。在许多情况下，子空间可能会发生扭曲和转动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 流形学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对训练实例所在的流形进行建模  \n",
    "依赖于流形假设：大多数现实世界的高维数据集都接近于低维流形  \n",
    "在训练模型之前降低训练集的维度肯定可以加快训练速度，但这并不总是会导致更好或更简单的解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最流行的降维算法  \n",
    "首先识别最靠近数据的超平面，然后将数据投影到其上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 保留差异性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集投影到低维超平面之前需要选择正确的超平面  \n",
    "选择保留最大差异性的轴看起来比较合理  \n",
    "比较原始数据集与其轴上的投影之间的均方距离，使这个最小的轴是最合理的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 主要成分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析可以在训练集中识别出哪条轴对差异性的贡献度最高   \n",
    "第i个轴称为数据的第i个主要成分(PC)  \n",
    "对于每个主要成分，PCA都找到一个指向PC方向的零中心单位向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值分解(SVD): \n",
    "将训练集矩形$X$分解为三个矩阵${U∑V}^T$的矩阵乘法，其中$V$包含定义所有主要成分的单位向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用NumPy的svd()函数来获取训练集的所有主要成分，然后提取定义前两个PC的两个单位向量\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=2000, noise=0.1)\n",
    "\n",
    "X_centered = X - X.mean(axis=0)     # 居中数据\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 向下投影到d维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定所有主要成分后，将数据集投影到前d个主要成分定义的超平面上，从而将数据集的维度降低到d维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集投影到d维度\n",
    "$$X{_{d-proj}}=XW{_d}$$  \n",
    "矩阵$W_d$定义为包含$V$的前$d$列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.4 使用Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将PCA转换器拟合到数据集后，其components_属性是$W_d$的转置  \n",
    "定义第一个主成分的单位向量等于pca.components_.T[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.5 可解释方差比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40036305, 0.31328611])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过explained_variance_ratio_变量来获得\n",
    "# 该比率表示每个成分的数据集方差的比率\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集方差的40%位于第一个PC上，而31%位于第二个PC上  \n",
    "对于第三个PC，不到29%，因此第三个PC携带的信息很少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.6 选择合适的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在不降低维度的情况下执行PCA, 然后计算保留95%训练集方差所需的最小维度：\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以设置n_components=d并再次运行PCA\n",
    "# 或将n_components设置为0.0到1.0之间的浮点数来表示要保留的方差率，而不是指定要保留的主成分数：\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.7 PCA压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据与重构数据(压缩后再解压缩)之间的均方距离称为重构误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将MNIST数据集压缩为154个维度，然后使用inverse_transform()方法将其解压缩回784个维度\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X_train = mnist[\"data\"]\n",
    "\n",
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.8 随机PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "快速找到前d个主成分的近似值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)\n",
    "\n",
    "# 默认设置为\"auto\", 如果m或n大于500并且d小于m或n的80%, 则Scikit-Learn自动使用随机PCA算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.9 增量PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把训练集划分为多个小批量，并一次将一个小批量送入IPCA算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将MNIST数据集拆分为100个小批量\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 内核PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内核:可以将实例隐式映射到一个高维空间(称为特征空间)，从而可以使用支持向量机来进行非线性分类和回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用于PCA，从而可以执行复杂的非线性投影来降低维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择内核并调整超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个两部流水线，首先使用kPCA将维度减少到二维，然后使用逻辑回归来分类\n",
    "# 使用GridSearchCV来查找kPCA的最佳内核和gamma值\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, t.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.03, 'kpca__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性降维技术  \n",
    "首先测量每个训练实例如何与其最近的邻居线性相关，然后寻找可以最好的保留这些局部关系的训练集的低维表示形式  \n",
    "适合展开扭曲的流形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.6 其他降维技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *随机投影*\n",
    "* *多维缩放(MDS)*\n",
    "* *Isomap*\n",
    "* *t分布随机近邻嵌入(t-SNE)*\n",
    "* *线性判别分析(LDA)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d99a3f7b344b3c3107482760db15f42178bfad658d282ab0a919b76809e13cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
