{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 维度的诅咒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 高维数据集很大可能是非常稀疏的：大多数训练实例可能彼此之间相距很远\n",
    "* 训练集的维度越高，过拟合的风险就越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 降维的主要方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有训练实例都位于(或接近于)高维空间的低维子空间内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "投影并不只是降低尺寸的最佳方法。在许多情况下，子空间可能会发生扭曲和转动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 流形学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对训练实例所在的流形进行建模  \n",
    "依赖于流形假设：大多数现实世界的高维数据集都接近于低维流形  \n",
    "在训练模型之前降低训练集的维度肯定可以加快训练速度，但这并不总是会导致更好或更简单的解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最流行的降维算法  \n",
    "首先识别最靠近数据的超平面，然后将数据投影到其上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 保留差异性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集投影到低维超平面之前需要选择正确的超平面  \n",
    "选择保留最大差异性的轴看起来比较合理  \n",
    "比较原始数据集与其轴上的投影之间的均方距离，使这个最小的轴是最合理的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 主要成分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析可以在训练集中识别出哪条轴对差异性的贡献度最高   \n",
    "第i个轴称为数据的第i个主要成分(PC)  \n",
    "对于每个主要成分，PCA都找到一个指向PC方向的零中心单位向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值分解(SVD): \n",
    "将训练集矩形$X$分解为三个矩阵${U∑V}^T$的矩阵乘法，其中$V$包含定义所有主要成分的单位向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用NumPy的svd()函数来获取训练集的所有主要成分，然后提取定义前两个PC的两个单位向量\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=2000, noise=0.1)\n",
    "\n",
    "X_centered = X - X.mean(axis=0)     # 居中数据\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 向下投影到d维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定所有主要成分后，将数据集投影到前d个主要成分定义的超平面上，从而将数据集的维度降低到d维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集投影到d维度\n",
    "$$X{_{d-proj}}=XW{_d}$$  \n",
    "矩阵$W_d$定义为包含$V$的前$d$列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.4 使用Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将PCA转换器拟合到数据集后，其components_属性是$W_d$的转置  \n",
    "定义第一个主成分的单位向量等于pca.components_.T[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.5 可解释方差比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d99a3f7b344b3c3107482760db15f42178bfad658d282ab0a919b76809e13cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
