{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一组预测器称为集成, 这种技术也被称为集成学习, 而一个集成学习算法则被称为集成方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 投票分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经训练好了一些分类器，要创建出一个更好的分类器，最简单的方法就是聚合每个分类器的预测，然后将得票最多的结果作为预测类别  \n",
    "这种大多数投票分类器被称为硬投票分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建并训练一个投票分类器，由三种不同的分类器组成，训练集是卫星数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8753333333333333\n",
      "RandomForestClassifier 0.988\n",
      "SVC 0.9896666666666667\n",
      "VotingClassifier 0.989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 bagging和pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging: 每个预测器使用的算法相同，但在不同的训练集随机子集上进行训练  \n",
    "pasting: 采样时样本不放回，bagging允许训练实例被同一个预测器多次采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与直接在原始训练集上训练的单个预测器相比，集成的偏差相近，但是方差更低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.1 Scikit-Learn中的bagging和pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个包含500个决策树分类器的集成\n",
    "# 每次从训练集随机采样100个训练实例，然后放回\n",
    "# 如果使用pasting，只需设置bootstrap=False\n",
    "# n_jobs指示多少CPU内核进行训练和预测(-1表示所有可用)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500,\n",
    "    max_samples=100, \n",
    "    bootstrap=True, \n",
    "    # n_jobs=-1         !会报错\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.2 包外评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未被采样的训练实例称为包外(oob)实例  \n",
    "设置oob_score=True在训练结束自动进行包外评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500,\n",
    "    max_samples=100, \n",
    "    bootstrap=True, \n",
    "    # n_jobs=-1         !会报错\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集上获得96.86%的准确率，与oob_score接近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32244898, 0.67755102],\n",
       "       [0.97160243, 0.02839757],\n",
       "       [0.        , 1.        ],\n",
       "       ...,\n",
       "       [0.95519348, 0.04480652],\n",
       "       [0.98985801, 0.01014199],\n",
       "       [0.98170732, 0.01829268]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个训练实例的包外决策函数也可以通过变量oob_decision_function_获得\n",
    "\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 随机补丁和随机子空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier类也支持对特征进行采样  \n",
    "采样由两个超参数控制:max_features和bootstrap_features  \n",
    "对高维输入(例如图像)特别有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是决策树的集合  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个拥有500棵树的随机森林分类器\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林在树的生长上引入了更多的随机性:分裂节点时不再是搜索最好的特征，而是在一个随机生成的特征子集里搜索最好的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.1 极端随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个特征使用随机阈值，而不是搜索得出的最佳阈值，则可能让决策树生长得更加随机  \n",
    "使用Scikit-learn的ExtraTreesClassifier类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.2 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10298648310206393\n",
      "sepal width (cm) 0.02822564894689312\n",
      "petal length (cm) 0.43100758506750814\n",
      "petal width (cm) 0.4377802828835348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 提升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升法(boosting)是指可以将几个弱学习器结合成一个强学习器的任意集成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.1 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新预测器对前序进行纠正的方法之一就是更多地关注前序欠拟合地训练实例，从而使新的预测器不断地越来越专注于难缠的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn的AdaBoostClassifier训练一个AdaBoost分类器，它基于200个单层决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",    # SAMME即等同于AdaBoost, SAMME.R是一种变体, 它依赖的是类概率而不是类预测, 通常表现更好\n",
    "    learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.2 梯度提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让新的预测器针对前一个预测器的残差进行拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度提升回归树(GBRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例\n",
    "# 首先在训练集上拟合一个DecisionTreeRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 针对第一个预测器的残差，训练第二个DecisionTreeRegressor\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 针对第二个预测器的残差，训练第三个DecisionTreeRegressor\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有树的预测相加，从而对新实例进行预测\n",
    "\n",
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练GBRT的简单方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# learning_rate对每棵树的贡献进行缩放，如果设为低值如0.1则需要更多的树来拟合训练集，但是预测的泛化效果通常更好\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要找到树的最佳数量，可以使用提前停止法  \n",
    "使用staged_predict()方法，在训练的每一阶段都对集成的预测器返回一个迭代器，然后测量每个训练阶段的验证误差，从而找到树的最优数量，最后使用最优树数重新训练一个GBRT集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)     # 初始120\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1    # 检索数组中最小值的位置，并返回其下标值\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现提前停止不一定需要先训练大量的树，然后再回头找最优的数字，还可以提前停止训练\n",
    "# 设置warm_start=True，从而允许增量训练\n",
    "\n",
    "# 验证误差连续5次迭代未改善，直接停止训练\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost  \n",
    "Author: Tianqi Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xbg_reg = xgboost.XGBRegressor()\n",
    "xbg_reg.fit(X_train, y_train)\n",
    "y_pred = xbg_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.35022\n",
      "[1]\tvalidation_0-rmse:0.24846\n",
      "[2]\tvalidation_0-rmse:0.17847\n",
      "[3]\tvalidation_0-rmse:0.13094\n",
      "[4]\tvalidation_0-rmse:0.09870\n",
      "[5]\tvalidation_0-rmse:0.07821\n",
      "[6]\tvalidation_0-rmse:0.06440\n",
      "[7]\tvalidation_0-rmse:0.05586\n",
      "[8]\tvalidation_0-rmse:0.05121\n",
      "[9]\tvalidation_0-rmse:0.04832\n",
      "[10]\tvalidation_0-rmse:0.04635\n",
      "[11]\tvalidation_0-rmse:0.04480\n",
      "[12]\tvalidation_0-rmse:0.04391\n",
      "[13]\tvalidation_0-rmse:0.04243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\李子豪\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\tvalidation_0-rmse:0.04150\n",
      "[15]\tvalidation_0-rmse:0.03633\n",
      "[16]\tvalidation_0-rmse:0.03318\n",
      "[17]\tvalidation_0-rmse:0.03295\n",
      "[18]\tvalidation_0-rmse:0.03161\n",
      "[19]\tvalidation_0-rmse:0.03084\n",
      "[20]\tvalidation_0-rmse:0.03062\n",
      "[21]\tvalidation_0-rmse:0.03050\n",
      "[22]\tvalidation_0-rmse:0.02987\n",
      "[23]\tvalidation_0-rmse:0.02941\n",
      "[24]\tvalidation_0-rmse:0.02932\n",
      "[25]\tvalidation_0-rmse:0.02816\n",
      "[26]\tvalidation_0-rmse:0.02759\n",
      "[27]\tvalidation_0-rmse:0.02682\n",
      "[28]\tvalidation_0-rmse:0.02648\n",
      "[29]\tvalidation_0-rmse:0.02632\n",
      "[30]\tvalidation_0-rmse:0.02558\n",
      "[31]\tvalidation_0-rmse:0.02445\n",
      "[32]\tvalidation_0-rmse:0.02403\n",
      "[33]\tvalidation_0-rmse:0.02344\n",
      "[34]\tvalidation_0-rmse:0.02245\n",
      "[35]\tvalidation_0-rmse:0.02226\n",
      "[36]\tvalidation_0-rmse:0.02210\n",
      "[37]\tvalidation_0-rmse:0.02143\n",
      "[38]\tvalidation_0-rmse:0.02071\n",
      "[39]\tvalidation_0-rmse:0.01978\n",
      "[40]\tvalidation_0-rmse:0.01928\n",
      "[41]\tvalidation_0-rmse:0.01916\n",
      "[42]\tvalidation_0-rmse:0.01861\n",
      "[43]\tvalidation_0-rmse:0.01791\n",
      "[44]\tvalidation_0-rmse:0.01730\n",
      "[45]\tvalidation_0-rmse:0.01613\n",
      "[46]\tvalidation_0-rmse:0.01586\n",
      "[47]\tvalidation_0-rmse:0.01546\n",
      "[48]\tvalidation_0-rmse:0.01536\n",
      "[49]\tvalidation_0-rmse:0.01449\n",
      "[50]\tvalidation_0-rmse:0.01443\n",
      "[51]\tvalidation_0-rmse:0.01386\n",
      "[52]\tvalidation_0-rmse:0.01366\n",
      "[53]\tvalidation_0-rmse:0.01320\n",
      "[54]\tvalidation_0-rmse:0.01292\n",
      "[55]\tvalidation_0-rmse:0.01267\n",
      "[56]\tvalidation_0-rmse:0.01244\n",
      "[57]\tvalidation_0-rmse:0.01237\n",
      "[58]\tvalidation_0-rmse:0.01204\n",
      "[59]\tvalidation_0-rmse:0.01201\n",
      "[60]\tvalidation_0-rmse:0.01187\n",
      "[61]\tvalidation_0-rmse:0.01175\n",
      "[62]\tvalidation_0-rmse:0.01171\n",
      "[63]\tvalidation_0-rmse:0.01124\n",
      "[64]\tvalidation_0-rmse:0.01088\n",
      "[65]\tvalidation_0-rmse:0.01069\n",
      "[66]\tvalidation_0-rmse:0.01066\n",
      "[67]\tvalidation_0-rmse:0.01063\n",
      "[68]\tvalidation_0-rmse:0.00994\n",
      "[69]\tvalidation_0-rmse:0.00953\n",
      "[70]\tvalidation_0-rmse:0.00940\n",
      "[71]\tvalidation_0-rmse:0.00929\n",
      "[72]\tvalidation_0-rmse:0.00908\n",
      "[73]\tvalidation_0-rmse:0.00870\n",
      "[74]\tvalidation_0-rmse:0.00858\n",
      "[75]\tvalidation_0-rmse:0.00813\n",
      "[76]\tvalidation_0-rmse:0.00790\n",
      "[77]\tvalidation_0-rmse:0.00787\n",
      "[78]\tvalidation_0-rmse:0.00718\n",
      "[79]\tvalidation_0-rmse:0.00693\n",
      "[80]\tvalidation_0-rmse:0.00679\n",
      "[81]\tvalidation_0-rmse:0.00676\n",
      "[82]\tvalidation_0-rmse:0.00660\n",
      "[83]\tvalidation_0-rmse:0.00578\n",
      "[84]\tvalidation_0-rmse:0.00564\n",
      "[85]\tvalidation_0-rmse:0.00555\n",
      "[86]\tvalidation_0-rmse:0.00518\n",
      "[87]\tvalidation_0-rmse:0.00496\n",
      "[88]\tvalidation_0-rmse:0.00487\n",
      "[89]\tvalidation_0-rmse:0.00349\n",
      "[90]\tvalidation_0-rmse:0.00345\n",
      "[91]\tvalidation_0-rmse:0.00344\n",
      "[92]\tvalidation_0-rmse:0.00289\n",
      "[93]\tvalidation_0-rmse:0.00276\n",
      "[94]\tvalidation_0-rmse:0.00266\n",
      "[95]\tvalidation_0-rmse:0.00246\n",
      "[96]\tvalidation_0-rmse:0.00220\n",
      "[97]\tvalidation_0-rmse:0.00181\n",
      "[98]\tvalidation_0-rmse:0.00163\n",
      "[99]\tvalidation_0-rmse:0.00000\n"
     ]
    }
   ],
   "source": [
    "# 自动提前停止\n",
    "\n",
    "xbg_reg.fit(X_train, y_train, eval_set=[(X_val, y_pred)], early_stopping_rounds=2)\n",
    "y_pred = xbg_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.6 堆叠法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用留存集  \n",
    "1. 将训练集分为两个子集，第一个子集用来训练第一层的预测器\n",
    "2. 第一层的预测器在第二个子集上进行预测\n",
    "3. 对于留存集中的每个实例都有了三个预测值，使用这些预测值创建一个新的训练集，并保留目标值\n",
    "4. 在这个新的训练集上训练混合器，让它学习根据第一层的预测来预测目标值"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d99a3f7b344b3c3107482760db15f42178bfad658d282ab0a919b76809e13cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
